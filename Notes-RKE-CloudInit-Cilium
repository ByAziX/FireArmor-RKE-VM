#!/bin/bash

# Variables

ID_TEMPLATE=1015
NAME_TEMPLATE="Worker-template"
RAM=4096
SOCKETS=1
CORES=2
SSH_KEY="/root/.ssh/id_rsa.pub"


# if fichier ubuntu-22.04-cloudinit-template exist
if [ -f "ubuntu-22.04-server-cloudimg-amd64-disk-kvm.img" ]; then
    echo "[TEMPLATE] file already exists"
else
    echo "[TEMPLATE] file not found"
    wget https://cloud-images.ubuntu.com/releases/22.04/release/ubuntu-22.04-server-cloudimg-amd64-disk-kvm.img
fi

# if il existe déjà un template sous le même nom et id on le supprime
if [ -f "/etc/pve/qemu-server/${ID_TEMPLATE}.conf" ]; then
    echo "[TEMPLATE] template already exists"
    exit 1
fi

echo "[TEMPLATE] create template"

# Create the instance
qm create ${ID_TEMPLATE} -name ${NAME_TEMPLATE} -memory ${RAM} -net0 virtio,bridge=vmbr0,firewall=1 -cores ${CORES} -sockets ${SOCKETS}

# Import the OpenStack disk image to Proxmox storage
qm importdisk ${ID_TEMPLATE} ubuntu-22.04-server-cloudimg-amd64-disk-kvm.img local-lvm

# Attach the disk to the virtual machine
qm set ${ID_TEMPLATE} -scsihw virtio-scsi-pci -scsi0 local-lvm:vm-${ID_TEMPLATE}-disk-0

qm set ${ID_TEMPLATE} -ipconfig0 ip=dhcp

# Set the bootdisk to the imported Openstack disk
qm set ${ID_TEMPLATE} -boot c -bootdisk scsi0

# Allow hotplugging of network, USB and disks
qm set ${ID_TEMPLATE} -hotplug disk,network,usb

# Enable the Qemu agent
qm set ${ID_TEMPLATE} -agent 1

# Add serial output and a video output
qm set ${ID_TEMPLATE} -serial0 socket -vga serial0

# Set a second hard drive, using the inbuilt cloudinit drive
qm set ${ID_TEMPLATE} -ide2 local-lvm:cloudinit

#qm set $ID_TEMPLATE -vmgenid 1
qm set $ID_TEMPLATE -ciuser Fire
qm set $ID_TEMPLATE -sshkey $SSH_KEY

# Enable cloud-init
#qm set ${ID_TEMPLATE} --cicustom "user=local:snippets/cloudinit.yaml"

# Resize the primary boot disk
qm resize ${ID_TEMPLATE} scsi0 +20G


echo "[TEMPLATE] set to template"

# Convert the VM to the template
#qm template ${ID_TEMPLATE}

echo "[TEMPLATE] Successful"


# mettre ca dans le template
https://github.com/justmeandopensource/kubernetes/tree/master/rancher/rke

do-release-upgrade

Install docker engine

https://docs.docker.com/engine/install/ubuntu/

https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository


sudo ufw disable

swapoff -a; sed -i '/swap/d' /etc/fstab

cat >>/etc/sysctl.d/kubernetes.conf<<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system

 sudo apt-get update

 sudo apt-get install \
    ca-certificates \
    curl \
    gnupg \
    lsb-release


    sudo mkdir -p /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg

echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
  $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

  sudo apt-get update

  sudo apt-get install docker-ce docker-ce-cli containerd.io docker-compose-plugin

  sudo docker run hello-world


sudo usermod -aG docker ${USER}

su - ${USER}

id -nG


sudo systemctl enable docker
sudo systemctl restart docker
sudo ufw allow 22,80,443,2376,2379,2380,4240,6443,5044,5601,9099,9345,10250,10254/tcp
sudo ufw allow 8472/udp
echo 'y' | sudo ufw enable
sudo -i
touch /etc/sysctl.d/kubernetes.conf
echo "net.bridge.bridge-nf-call-iptables = 1" > /etc/sysctl.d/kubernetes.conf
echo "net.bridge.bridge-nf-call-ip6tables = 1" >> /etc/sysctl.d/kubernetes.conf 
sysctl --system

## enlever le mếme id des vm

sudo cloud-init clean
sudo rm -rf /var/lib/cloud/instances
sudo truncate -s 0 /etc/machine-id
sudo rm /var/lib/dbus/machine-id
sudo ln -s /etc/machine-id /var/lib/dbus/machine-id
ls -l /var/lib/dbus/machine-id
cat /etc/machine-id










curl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION="v1.26.0%2Brke2r2" sh -


# RKE2 

https://gitlab.com/monachus/channel/-/blob/master/resources/2021-09-07-ha-rke2-kube-vip-rancher/README.md#user-content-ha-kubernetes-with-rke2-kube-vip-and-rancher


cat > master.yaml<<'EOF'
tls-san:
- master1
- 10.10.10.175
disable:
  - rke2-canal
  - rke2-ingress-nginx
  - rke2-kube-proxy
cni:
- cilium
EOF


sudo -s

#curl -sfL https://get.rke2.io | sh -


# RKE2
mkdir -p /etc/rancher/rke2
cp master.yaml /etc/rancher/rke2/config.yaml
curl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION="v1.26.0%2Brke2r2" sh -
systemctl enable rke2-server
systemctl start rke2-server
# wait for rke2 to be ready

# 
export VIP=10.10.10.175
export TAG=v0.3.8
export INTERFACE=eth0
export CONTAINER_RUNTIME_ENDPOINT=unix:///run/k3s/containerd/containerd.sock
export CONTAINERD_ADDRESS=/run/k3s/containerd/containerd.sock
export PATH=/var/lib/rancher/rke2/bin:$PATH
export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
alias k=kubectl
ln -fs /var/lib/rancher/rke2/bin/{kubectl,crictl,ctr} /usr/local/bin/
kubectl get nodes


cat >/etc/profile.d/rke2.sh <<'EOF'
export CONTAINER_RUNTIME_ENDPOINT=unix:///run/k3s/containerd/containerd.sock
export CONTAINERD_ADDRESS=/run/k3s/containerd/containerd.sock
export PATH=/var/lib/rancher/rke2/bin:$PATH
export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
alias k=kubectl
EOF
source /etc/profile.d/rke2.sh
kubectl get nodes -o wide

# Kube-VIP Installation

# Set up environment

curl -s https://kube-vip.io/manifests/rbac.yaml > /var/lib/rancher/rke2/server/manifests/kube-vip-rbac.yaml
crictl pull docker.io/plndr/kube-vip:$TAG
alias kube-vip="ctr --namespace k8s.io run --rm --net-host docker.io/plndr/kube-vip:$TAG vip /kube-vip"
kube-vip manifest daemonset \
    --arp \
    --interface $INTERFACE \
    --address $VIP \
    --controlplane \
    --leaderElection \
    --taint \
    --services \
    --inCluster | tee /var/lib/rancher/rke2/server/manifests/kube-vip.yaml

## check logs

```k get po -n kube-system | grep kube-vip```

>root@demo-a:~# k get po -n kube-system | grep kube-vip
>kube-vip-ds-8595m                           1/1     Running     0          48s

```k logs kube-vip-ds-9lmkl -n kube-system --tail 1```

>root@demo-a:~# k logs kube-vip-ds-8595m -n kube-system --tail 1
>time="2023-01-15T12:18:34Z" level=info msg="Broadcasting ARP update for 10.10.10.175 (be:40:66:0a:87:f9) via eth0"

ping $VIP

>PING 10.10.10.175 (10.10.10.175) 56(84) bytes of data.
>64 bytes from 10.10.10.175: icmp_seq=1 ttl=64 time=0.198 ms
>64 bytes from 10.10.10.175: icmp_seq=2 ttl=64 time=0.039 ms


cat /var/lib/rancher/rke2/server/node-token

# master 2

cat > master.yaml<<'EOF'
token: K1072d3bd38fb9dcfd7283ff63bcec4b2cf9aab7e9150871a5450abc69aabb30296::server:24ff086eee4c88db2a603250b4ddc0f8
server: https://10.10.10.175:9345
tls-san:
- 10.10.10.175
disable: 
  - rke2-ingress-nginx
  - rke2-kube-proxy
cni:
- cilium
EOF

# Set up environment

export VIP=10.10.10.175
export TAG=v0.3.8
export INTERFACE=eth0
export CONTAINER_RUNTIME_ENDPOINT=unix:///run/k3s/containerd/containerd.sock
export CONTAINERD_ADDRESS=/run/k3s/containerd/containerd.sock
export PATH=/var/lib/rancher/rke2/bin:$PATH
export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
alias k=kubectl

mkdir -p /etc/rancher/rke2
cp master.yaml /etc/rancher/rke2/config.yaml
curl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION="v1.26.0%2Brke2r2" sh -
systemctl enable rke2-server
systemctl start rke2-server
kubectl get nodes





master1 :
kubectl get nodes -w
k get po -n kube-system | grep kube-vip
k logs kube-vip-ds-9lmkl -n kube-system --tail 1

master2:

cp /etc/rancher/rke2/rke2.yaml .
nano rke2.yaml

change 127.0.0.1:6443 -> 10.10.10.175

k --kubeconfig ./rke2.yaml get no

# IP

Master1: 10.10.10.148
Master2: 10.10.10.49
Master3: 10.10.10.76
Worker4: 10.10.10.101
Worker5: 10.10.10.32
kube-vip : 10.10.10.175 ip virtuel



# Worker 

cat > worker.yaml<<'EOF'
token: K1072d3bd38fb9dcfd7283ff63bcec4b2cf9aab7e9150871a5450abc69aabb30296::server:24ff086eee4c88db2a603250b4ddc0f8
server: https://10.10.10.175:9345
tls-san:
- 10.10.10.175
disable: 
  - rke2-ingress-nginx
  - rke2-kube-proxy
cni:
- cilium
EOF


#export VIP=10.10.10.175
#export TAG=v0.3.8
#export INTERFACE=eth0
#export CONTAINER_RUNTIME_ENDPOINT=unix:///run/k3s/containerd/containerd.sock
#export CONTAINERD_ADDRESS=/run/k3s/containerd/containerd.sock
#export PATH=/var/lib/rancher/rke2/bin:$PATH
#export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
#alias k=kubectl

sudo -s

mkdir -p /etc/rancher/rke2
cp worker.yaml /etc/rancher/rke2/config.yaml
curl -sfL https://get.rke2.io | INSTALL_RKE2_TYPE="agent" INSTALL_RKE2_VERSION="v1.26.0%2Brke2r2" sh -
systemctl start rke2-agent.service



# cilium check 

curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz{,.sha256sum}
sha256sum --check cilium-linux-amd64.tar.gz.sha256sum
sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin

cilium status




# cloud-provider LOADBalancer

# demo-a
curl -sfL https://raw.githubusercontent.com/kube-vip/kube-vip-cloud-provider/main/manifest/kube-vip-cloud-controller.yaml > /var/lib/rancher/rke2/server/manifests/kube-vip-cloud-controller.yaml

# copy configmap into place
cat > kube-vip-config.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: kubevip
  namespace: kube-system
data:
  range-global: 10.10.10.176-10.10.10.186

cp kube-vip-config.yaml /var/lib/rancher/rke2/server/manifests

# test
kubectl create deploy nginx --image=nginx:stable-alpine
kubectl expose deploy nginx --port=80 --type=LoadBalancer

kubectl get services




kubectl delete --all services --namespace=nginx
kubectl delete deployment deployment-name



# Get commands with basic output
kubectl get services                          # List all services in the namespace
kubectl get pods --all-namespaces             # List all pods in all namespaces
kubectl get pods -o wide                      # List all pods in the current namespace, with more details
kubectl get deployment my-dep                 # List a particular deployment
kubectl get pod my-pod -o yaml                # Get a pod's YAML
kubectl -n kube-system delete pod rke2-coredns-rke2-coredns-58fd75f64b-99mtj





helm repo add cilium https://helm.cilium.io
helm upgrade --install cilium cilium/cilium --version 1.11.1 --namespace kube-system --set kubeProxyReplacement=strict --set k8sServiceHost="127.0.0.1" --set k8sServicePort=6443 --set loadBalancer.serviceTopology=true --set ipam.mode="kubernetes"


kubectl get pods --all-namespaces -o custom-columns=NAMESPACE:.metadata.namespace,NAME:.metadata.name,HOSTNETWORK:.spec.hostNetwork --no-headers=true | grep '<none>' | awk '{print "-n "$1" "$2}' | xargs -L 1 -r kubectl delete pod







#Probleme :
iptables -S
open port 9354 : sudo ufw allow 9345/tcp


Feb 06 19:20:34 master2 sh[8055]: + /usr/bin/systemctl is-enabled --quiet nm-cloud-setup.service
Feb 06 19:20:34 master2 sh[8055]: /bin/sh: 1: /usr/bin/systemctl: not found
Feb 06 19:20:35 master2 rke2[8070]: time="2023-02-06T19:20:35Z" level=warning msg="not running in CIS mode"
Feb 06 19:20:35 master2 rke2[8070]: time="2023-02-06T19:20:35Z" level=info msg="Starting rke2 v1.24.10+rke2r1 (1ccdce2571291649b9414af1f269f645c3fe4002)"
Feb 06 19:20:55 master2 rke2[8070]: time="2023-02-06T19:20:55Z" level=fatal msg="starting kubernetes: preparing server: failed to get CA certs: Get \"https://10.10.10.175:9345/cacerts\": context deadline exceeded (Client.Timeout exceede"
Feb 06 19:20:55 master2 systemd[1]: rke2-server.service: Main process exited, code=exited, status=1/FAILURE
Feb 06 19:20:55 master2 systemd[1]: rke2-server.service: Failed with result 'exit-code'.
Feb 06 19:20:55 master2 systemd[1]: Failed to start Rancher Kubernetes Engine v2 (server).









# cilium 
https://docs.cilium.io/en/v1.12/gettingstarted/k8s-install-helm/
https://www.puzzle.ch/de/blog/articles/2020/12/23/cilium-on-rancher

https://docs.rke2.io/install/network_options/
https://fossies.org/linux/cilium/Documentation/gettingstarted/k8s-install-rke.rst

cluster.yml

network:
  plugin: none


sudo curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION="v1.20.15+k3s1" K3S_KUBECONFIG_MODE="644" INSTALL_K3S_EXEC=" --disable traefik --flannel-backend=none --disable-network-policy --cluster-init" sh -


curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz{,.sha256sum}
sha256sum --check cilium-linux-amd64.tar.gz.sha256sum
sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin
export KUBECONFIG=~/.kube/config

cilium install
cilium status --wait
kubectl get all -A -o wide











# cilium connectivity test

cilium connectivity test


# si probleme de migration sur proxmox 

# sur chaque node proxmox
sh-copy-id -o 'HostKeyAlias=pve1' root@10.10.10.16
sh-copy-id -o 'HostKeyAlias=pve2' root@10.10.10.15
sh-copy-id -o 'HostKeyAlias=pve3' root@10.10.10.14
sh-copy-id -o 'HostKeyAlias=pve4' root@10.10.10.13
sh-copy-id -o 'HostKeyAlias=pve5' root@10.10.10.12
















# Kubeadm
https://www.bookstack.cn/read/cilium-1.12-en/b923ab7d137d9690.md

si bug en point :

rm /etc/containerd/config.toml
systemctl restart containerd
kubeadm init


sudo apt-get update && sudo apt-get install -y apt-transport-https curl
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl




kubeadm init --skip-phases=addon/kube-proxy


mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

regular user :
mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

sudo user : 
export KUBECONFIG=/etc/kubernetes/admin.conf










## sur les nodes si ya un probleme avec le tunneling

sudo chmod 666 /var/run/docker.sock

## probleme sur I'm getting the below exception. when I'm executing the kubectl command in local
E1215 22:14:12.237942 41787 memcache.go:238] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused
E1215 22:14:12.238652 41787 memcache.go:238] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused
E1215 22:14:12.239633 41787 memcache.go:238] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused
E1215 22:14:12.240716 41787 memcache.go:238] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused
E1215 22:14:12.241875 41787 memcache.go:238] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused
The connection to the server localhost:8080 was refused - did you specify the right host or port?



alors : il trouve pas fichier config

export KUBECONFIG=~/.kube/config














# sur le host
/etc/ssh/sshd_config


AllowTcpForwarding yes
AllowStreamLocalForwarding yes
DisableForwarding no

systemctl restart sshd
https://rancher.com/docs/rke/latest/en/managing-clusters/#:~:text=RKE%20supports%20adding%2Fremoving%20nodes,list%20in%20the%20original%20cluster


https://fossies.org/linux/cilium/Documentation/gettingstarted/k8s-install-rke.rst



# You can monitor as Cilium and all required components are being installed:

kubectl -n kube-system get pods --watch



kubectl create ns cilium-test
kubectl get pods -n cilium-test
kubectl delete ns cilium-test




# commande kub 

rke up

kubectl cluster-info

kubectl config get-contexts


# deployment 


rke up --config ./cluster.yml

 

# remove un cluster 

rke remove

rke up

rke up --config ./cluster.yml

mv kube_config_cluster.yml ~/.kube/config

export KUBECONFIG=~/.kube/config

kubectl get nodes

# remove un node

kubectl delete node


# PROBLEME SI ON CHOISIE L'IP DE LA VM

DONC LAISSER DHCP

# IP vm
10.10.10.61
10.10.10.138
10.10.10.147
10.10.10.133
10.10.10.154


# If you intended to deploy Kubernetes in an air-gapped environment,
# please consult the documentation on how to configure custom RKE images.
nodes:
- address: 10.10.10.105
  port: "22"
  internal_address: 10.10.10.105
  role:
  - controlplane
  - worker
  - etcd
  hostname_override: node1
  user: Fire
  docker_socket: /var/run/docker.sock
  ssh_key: ""
  ssh_key_path: ~/.ssh/id_rsa
  ssh_cert: ""
  ssh_cert_path: ""
  labels: {}
  taints: []
services:
  etcd:
    image: ""
    extra_args: {}
    extra_args_array: {}
    extra_binds: []
    extra_env: []
    win_extra_args: {}
    win_extra_args_array: {}
    win_extra_binds: []
    win_extra_env: []
    external_urls: []
    ca_cert: ""
    cert: ""
    key: ""
    path: ""
    uid: 0
    gid: 0
    snapshot: null
    retention: ""
    creation: ""
    backup_config: null
  kube-api:
    image: ""
    extra_args: {}
    extra_args_array: {}
    extra_binds: []
    extra_env: []
    win_extra_args: {}
    win_extra_args_array: {}
    win_extra_binds: []
    win_extra_env: []
    service_cluster_ip_range: 10.43.0.0/16
    service_node_port_range: ""
    pod_security_policy: false
    always_pull_images: false
    secrets_encryption_config: null
    audit_log: null
    admission_configuration: null
    event_rate_limit: null
  kube-controller:
    image: ""
    extra_args: {}
    extra_args_array: {}
    extra_binds: []
    extra_env: []
    win_extra_args: {}
    win_extra_args_array: {}
    win_extra_binds: []
    win_extra_env: []
    cluster_cidr: 10.42.0.0/16
    service_cluster_ip_range: 10.43.0.0/16
  scheduler:
    image: ""
    extra_args: {}
    extra_args_array: {}
    extra_binds: []
    extra_env: []
    win_extra_args: {}
    win_extra_args_array: {}
    win_extra_binds: []
    win_extra_env: []
  kubelet:
    image: ""
    extra_args: {}
    extra_args_array: {}
    extra_binds: []
    extra_env: []
    win_extra_args: {}
    win_extra_args_array: {}
    win_extra_binds: []
    win_extra_env: []
    cluster_domain: cluster.local
    infra_container_image: ""
    cluster_dns_server: 10.43.0.10
    fail_swap_on: false
    generate_serving_certificate: false
  kubeproxy:
    image: ""
    extra_args: {}
    extra_args_array: {}
    extra_binds: []
    extra_env: []
    win_extra_args: {}
    win_extra_args_array: {}
    win_extra_binds: []
    win_extra_env: []
network:
  plugin: calico
  options: {}
  mtu: 0
  node_selector: {}
  update_strategy: null
  tolerations: []
authentication:
  strategy: x509
  sans: []
  webhook: null
addons: ""
addons_include: []
system_images:
  etcd: rancher/mirrored-coreos-etcd:v3.5.4
  alpine: rancher/rke-tools:v0.1.88
  nginx_proxy: rancher/rke-tools:v0.1.88
  cert_downloader: rancher/rke-tools:v0.1.88
  kubernetes_services_sidecar: rancher/rke-tools:v0.1.88
  kubedns: rancher/mirrored-k8s-dns-kube-dns:1.21.1
  dnsmasq: rancher/mirrored-k8s-dns-dnsmasq-nanny:1.21.1
  kubedns_sidecar: rancher/mirrored-k8s-dns-sidecar:1.21.1
  kubedns_autoscaler: rancher/mirrored-cluster-proportional-autoscaler:1.8.5
  coredns: rancher/mirrored-coredns-coredns:1.9.3
  coredns_autoscaler: rancher/mirrored-cluster-proportional-autoscaler:1.8.5
  nodelocal: rancher/mirrored-k8s-dns-node-cache:1.21.1
  kubernetes: rancher/hyperkube:v1.24.8-rancher1
  flannel: rancher/mirrored-coreos-flannel:v0.15.1
  flannel_cni: rancher/flannel-cni:v0.3.0-rancher6
  calico_node: rancher/mirrored-calico-node:v3.22.0
  calico_cni: rancher/calico-cni:v3.22.0-rancher1
  calico_controllers: rancher/mirrored-calico-kube-controllers:v3.22.0
  calico_ctl: rancher/mirrored-calico-ctl:v3.22.0
  calico_flexvol: rancher/mirrored-calico-pod2daemon-flexvol:v3.22.0
  canal_node: rancher/mirrored-calico-node:v3.22.0
  canal_cni: rancher/calico-cni:v3.22.0-rancher1
  canal_controllers: rancher/mirrored-calico-kube-controllers:v3.22.0
  canal_flannel: rancher/mirrored-flannelcni-flannel:v0.17.0
  canal_flexvol: rancher/mirrored-calico-pod2daemon-flexvol:v3.22.0
  weave_node: weaveworks/weave-kube:2.8.1
  weave_cni: weaveworks/weave-npc:2.8.1
  pod_infra_container: rancher/mirrored-pause:3.6
  ingress: rancher/nginx-ingress-controller:nginx-1.2.1-rancher1
  ingress_backend: rancher/mirrored-nginx-ingress-controller-defaultbackend:1.5-rancher1
  ingress_webhook: rancher/mirrored-ingress-nginx-kube-webhook-certgen:v1.1.1
  metrics_server: rancher/mirrored-metrics-server:v0.6.1
  windows_pod_infra_container: rancher/mirrored-pause:3.6
  aci_cni_deploy_container: noiro/cnideploy:5.2.3.4.1d150da
  aci_host_container: noiro/aci-containers-host:5.2.3.4.1d150da
  aci_opflex_container: noiro/opflex:5.2.3.4.1d150da
  aci_mcast_container: noiro/opflex:5.2.3.4.1d150da
  aci_ovs_container: noiro/openvswitch:5.2.3.4.1d150da
  aci_controller_container: noiro/aci-containers-controller:5.2.3.4.1d150da
  aci_gbp_server_container: noiro/gbp-server:5.2.3.4.1d150da
  aci_opflex_server_container: noiro/opflex-server:5.2.3.4.1d150da
ssh_key_path: ~/.ssh/id_rsa
ssh_cert_path: ""
ssh_agent_auth: false
authorization:
  mode: rbac
  options: {}
ignore_docker_version: null
enable_cri_dockerd: null
kubernetes_version: ""
private_registries: []
ingress:
  provider: ""
  options: {}
  node_selector: {}
  extra_args: {}
  dns_policy: ""
  extra_envs: []
  extra_volumes: []
  extra_volume_mounts: []
  update_strategy: null
  http_port: 0
  https_port: 0
  network_mode: ""
  tolerations: []
  default_backend: null
  default_http_backend_priority_class_name: ""
  nginx_ingress_controller_priority_class_name: ""
  default_ingress_class: null
cluster_name: ""
cloud_provider:
  name: ""
prefix_path: ""
win_prefix_path: ""
addon_job_timeout: 0
bastion_host:
  address: ""
  port: ""
  user: ""
  ssh_key: ""
  ssh_key_path: ""
  ssh_cert: ""
  ssh_cert_path: ""
  ignore_proxy_env_vars: false
monitoring:
  provider: ""
  options: {}
  node_selector: {}
  update_strategy: null
  replicas: null
  tolerations: []
  metrics_server_priority_class_name: ""
restore:
  restore: false
  snapshot_name: ""
rotate_encryption_key: false
dns: null



















nodes:
    - address: 1.1.1.1
      user: ubuntu
      role:
        - controlplane
        - etcd
      port: 2222
      docker_socket: /var/run/docker.sock
    - address: 2.2.2.2
      user: ubuntu
      role:
        - worker
      ssh_key_path: /home/user/.ssh/id_rsa
      ssh_key: |-
        -----BEGIN RSA PRIVATE KEY-----

        -----END RSA PRIVATE KEY-----
      ssh_cert_path: /home/user/.ssh/test-key-cert.pub
      ssh_cert: |-
        ssh-rsa-cert-v01@openssh.com AAAAHHNzaC1yc2EtY2VydC12MDFAb3Bl....
    - address: example.com
      user: ubuntu
      role:
        - worker
      hostname_override: node3
      internal_address: 192.168.1.6
      labels:
        app: ingress
      taints:
        - key: test-key
          value: test-value
          effect: NoSchedule

# If set to true, RKE will not fail when unsupported Docker version
# are found
ignore_docker_version: false

# Enable running cri-dockerd
# Up to Kubernetes 1.23, kubelet contained code called dockershim 
# to support Docker runtime. The replacement is called cri-dockerd 
# and should be enabled if you want to keep using Docker as your
# container runtime
# Only available to enable in Kubernetes 1.21 and higher
enable_cri_dockerd: true

# Cluster level SSH private key
# Used if no ssh information is set for the node
ssh_key_path: ~/.ssh/test

# Enable use of SSH agent to use SSH private keys with passphrase
# This requires the environment `SSH_AUTH_SOCK` configured pointing
#to your SSH agent which has the private key added
ssh_agent_auth: true

# List of registry credentials
# If you are using a Docker Hub registry, you can omit the `url`
# or set it to `docker.io`
# is_default set to `true` will override the system default
# registry set in the global settings
private_registries:
     - url: registry.com
       user: Username
       password: password
       is_default: true

# Bastion/Jump host configuration
bastion_host:
    address: x.x.x.x
    user: ubuntu
    port: 22
    ssh_key_path: /home/user/.ssh/bastion_rsa
# or
#   ssh_key: |-
#     -----BEGIN RSA PRIVATE KEY-----
#
#     -----END RSA PRIVATE KEY-----

# Set the name of the Kubernetes cluster  
cluster_name: mycluster


# The Kubernetes version used. The default versions of Kubernetes
# are tied to specific versions of the system images.
#
# For RKE v0.2.x and below, the map of Kubernetes versions and their system images is
# located here:
# https://github.com/rancher/types/blob/release/v2.2/apis/management.cattle.io/v3/k8s_defaults.go
#
# For RKE v0.3.0 and above, the map of Kubernetes versions and their system images is
# located here:
# https://github.com/rancher/kontainer-driver-metadata/blob/master/rke/k8s_rke_system_images.go
#
# In case the kubernetes_version and kubernetes image in
# system_images are defined, the system_images configuration
# will take precedence over kubernetes_version.
kubernetes_version: v1.10.3-rancher2

# System Images are defaulted to a tag that is mapped to a specific
# Kubernetes Version and not required in a cluster.yml. 
# Each individual system image can be specified if you want to use a different tag.
#
# For RKE v0.2.x and below, the map of Kubernetes versions and their system images is
# located here:
# https://github.com/rancher/types/blob/release/v2.2/apis/management.cattle.io/v3/k8s_defaults.go
#
# For RKE v0.3.0 and above, the map of Kubernetes versions and their system images is
# located here:
# https://github.com/rancher/kontainer-driver-metadata/blob/master/rke/k8s_rke_system_images.go
#
system_images:
    kubernetes: rancher/hyperkube:v1.10.3-rancher2
    etcd: rancher/coreos-etcd:v3.1.12
    alpine: rancher/rke-tools:v0.1.9
    nginx_proxy: rancher/rke-tools:v0.1.9
    cert_downloader: rancher/rke-tools:v0.1.9
    kubernetes_services_sidecar: rancher/rke-tools:v0.1.9
    kubedns: rancher/k8s-dns-kube-dns-amd64:1.14.8
    dnsmasq: rancher/k8s-dns-dnsmasq-nanny-amd64:1.14.8
    kubedns_sidecar: rancher/k8s-dns-sidecar-amd64:1.14.8
    kubedns_autoscaler: rancher/cluster-proportional-autoscaler-amd64:1.0.0
    pod_infra_container: rancher/pause-amd64:3.1

services:
    etcd:
      backup_config:
        interval_hours: 12
        retention: 6
        s3backupconfig:
          access_key: S3_ACCESS_KEY
          secret_key: S3_SECRET_KEY
          bucket_name: s3-bucket-name
          region: ""
          folder: "" # Optional - Available as of v0.3.0
          endpoint: s3.amazonaws.com
          custom_ca: |-
            -----BEGIN CERTIFICATE-----
            $CERTIFICATE
            -----END CERTIFICATE-----
          
      # Custom uid/guid for etcd directory and files
      uid: 52034
      gid: 52034
      # if external etcd is used
      # path: /etcdcluster
      # external_urls:
      #   - https://etcd-example.com:2379
      # ca_cert: |-
      #   -----BEGIN CERTIFICATE-----
      #   xxxxxxxxxx
      #   -----END CERTIFICATE-----
      # cert: |-
      #   -----BEGIN CERTIFICATE-----
      #   xxxxxxxxxx
      #   -----END CERTIFICATE-----
      # key: |-
      #   -----BEGIN PRIVATE KEY-----
      #   xxxxxxxxxx
      #   -----END PRIVATE KEY-----
    # Note for Rancher v2.0.5 and v2.0.6 users: If you are configuring
    # Cluster Options using a Config File when creating Rancher Launched
    # Kubernetes, the names of services should contain underscores
    # only: `kube_api`.
    kube-api:
      # IP range for any services created on Kubernetes
      # This must match the service_cluster_ip_range in kube-controller
      service_cluster_ip_range: 10.43.0.0/16
      # Expose a different port range for NodePort services
      service_node_port_range: 30000-32767    
      pod_security_policy: false
      # Encrypt secret data at Rest
      # Available as of v0.3.1
      secrets_encryption_config:
        enabled: true
        custom_config:
          apiVersion: apiserver.config.k8s.io/v1
          kind: EncryptionConfiguration
          resources:
          - resources:
            - secrets
            providers:
            - aescbc:
                keys:
                - name: k-fw5hn
                  secret: RTczRjFDODMwQzAyMDVBREU4NDJBMUZFNDhCNzM5N0I=
            - identity: {}
      # Enable audit logging
      # Available as of v1.0.0
      audit_log:
        enabled: true
        configuration:
          max_age: 6
          max_backup: 6
          max_size: 110
          path: /var/log/kube-audit/audit-log.json
          format: json
          policy:
            apiVersion: audit.k8s.io/v1 # This is required.
            kind: Policy
            omitStages:
              - "RequestReceived"
            rules:
              # Log pod changes at RequestResponse level
              - level: RequestResponse
                resources:
                - group: ""
                  # Resource "pods" doesn't match requests to any subresource of pods,
                  # which is consistent with the RBAC policy.
                  resources: ["pods"]
      # Using the EventRateLimit admission control enforces a limit on the number of events
      # that the API Server will accept in a given time period
      # Available as of v1.0.0
      event_rate_limit:
        enabled: true
        configuration:
          apiVersion: eventratelimit.admission.k8s.io/v1alpha1
          kind: Configuration
          limits:
          - type: Server
            qps: 6000
            burst: 30000
      # Enable AlwaysPullImages Admission controller plugin
      # Available as of v0.2.0
      always_pull_images: false
      # Add additional arguments to the kubernetes API server
      # This WILL OVERRIDE any existing defaults
      extra_args:
        # Enable audit log to stdout
        audit-log-path: "-"
        # Increase number of delete workers
        delete-collection-workers: 3
        # Set the level of log output to debug-level
        v: 4
    # Note for Rancher 2 users: If you are configuring Cluster Options
    # using a Config File when creating Rancher Launched Kubernetes,
    # the names of services should contain underscores only:
    # `kube_controller`. This only applies to Rancher v2.0.5 and v2.0.6.
    kube-controller:
      # CIDR pool used to assign IP addresses to pods in the cluster
      cluster_cidr: 10.42.0.0/16
      # IP range for any services created on Kubernetes
      # This must match the service_cluster_ip_range in kube-api
      service_cluster_ip_range: 10.43.0.0/16
      # Add additional arguments to the kubernetes API server
      # This WILL OVERRIDE any existing defaults
      extra_args:
        # Set the level of log output to debug-level
        v: 4
        # Enable RotateKubeletServerCertificate feature gate
        feature-gates: RotateKubeletServerCertificate=true
        # Enable TLS Certificates management
        # https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/
        cluster-signing-cert-file: "/etc/kubernetes/ssl/kube-ca.pem"
        cluster-signing-key-file: "/etc/kubernetes/ssl/kube-ca-key.pem"
        # Change the per-node pod subnet size for more or larger nodes, default is /24
        node-cidr-mask-size: '24'
    kubelet:
      # Base domain for the cluster
      cluster_domain: cluster.local
      # IP address for the DNS service endpoint
      cluster_dns_server: 10.43.0.10
      # Fail if swap is on
      fail_swap_on: false
      # Configure pod-infra-container-image argument
      pod-infra-container-image: "k8s.gcr.io/pause:3.2"
      # Generate a certificate signed by the kube-ca Certificate Authority
      # for the kubelet to use as a server certificate
      # Available as of v1.0.0
      generate_serving_certificate: true
      extra_args:
        # Set max pods to 250 instead of default 110
        max-pods: 250
        # Enable RotateKubeletServerCertificate feature gate
        feature-gates: RotateKubeletServerCertificate=true
      # Optionally define additional volume binds to a service
      extra_binds:
        - "/usr/libexec/kubernetes/kubelet-plugins:/usr/libexec/kubernetes/kubelet-plugins"
    scheduler:
      extra_args:
        # Set the level of log output to debug-level
        v: 4
    kubeproxy:
      extra_args:
        # Set the level of log output to debug-level
        v: 4

# Currently, only authentication strategy supported is x509.
# You can optionally create additional SANs (hostnames or IPs) to
# add to the API server PKI certificate.
# This is useful if you want to use a load balancer for the
# control plane servers.
authentication:
    strategy: x509
    sans:
      - "10.18.160.10"
      - "my-loadbalancer-1234567890.us-west-2.elb.amazonaws.com"

# Kubernetes Authorization mode
# Use `mode: rbac` to enable RBAC
# Use `mode: none` to disable authorization
authorization:
    mode: rbac

# If you want to set a Kubernetes cloud provider, you specify
# the name and configuration
cloud_provider:
    name: aws

# Add-ons are deployed using kubernetes jobs. RKE will give
# up on trying to get the job status after this timeout in seconds..
addon_job_timeout: 30

# Specify network plugin-in (canal, calico, flannel, weave, or none)
network:
  plugin: canal
  # Specify MTU
  mtu: 1400
  options:
    # Configure interface to use for Canal
    canal_iface: eth1
    canal_flannel_backend_type: vxlan
    # Available as of v1.2.6
    canal_autoscaler_priority_class_name: system-cluster-critical
    canal_priority_class_name: system-cluster-critical
  # Available as of v1.2.4
  tolerations:
  - key: "node.kubernetes.io/unreachable"
    operator: "Exists"
    effect: "NoExecute"
    tolerationseconds: 300
  - key: "node.kubernetes.io/not-ready"
    operator: "Exists"
    effect: "NoExecute"
    tolerationseconds: 300
  # Available as of v1.1.0
  update_strategy:
    strategy: RollingUpdate
    rollingUpdate:
      maxUnavailable: 6

# Specify DNS provider (coredns or kube-dns)
dns:
  provider: coredns
  # Available as of v1.1.0
  update_strategy:
    strategy: RollingUpdate
    rollingUpdate:
      maxUnavailable: 20%
      maxSurge: 15%
  linear_autoscaler_params:
    cores_per_replica: 0.34
    nodes_per_replica: 4
    prevent_single_point_failure: true
    min: 2
    max: 3

# Specify monitoring provider (metrics-server)
monitoring:
  provider: metrics-server
  # Available as of v1.1.0
  update_strategy:
    strategy: RollingUpdate
    rollingUpdate:
      maxUnavailable: 8

# Currently only nginx ingress provider is supported.
# To disable ingress controller, set `provider: none`
# `node_selector` controls ingress placement and is optional
ingress:
  provider: nginx
  node_selector:
    app: ingress
  # Available as of v1.1.0
  update_strategy:
    strategy: RollingUpdate
    rollingUpdate:
      maxUnavailable: 5
      
# All add-on manifests MUST specify a namespace
addons: |-
    ---
    apiVersion: v1
    kind: Pod
    metadata:
      name: my-nginx
      namespace: default
    spec:
      containers:
      - name: my-nginx
        image: nginx
        ports:
        - containerPort: 80

addons_include:
    - https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/rook-operator.yaml
    - https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/rook-cluster.yaml
    - /path/to/manifest